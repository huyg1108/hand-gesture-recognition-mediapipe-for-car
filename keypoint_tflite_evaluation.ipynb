{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f407afbb",
   "metadata": {},
   "source": [
    "# TensorFlow Lite Model Performance Evaluation Report\n",
    "\n",
    "---\n",
    "\n",
    "### üìã **Th√¥ng tin ƒë·ªì √°n**\n",
    "- **Project**: Hand Gesture Recognition for Car Control System\n",
    "- **Model**: TensorFlow Lite Keypoint Classifier\n",
    "- **Author**: Nh√≥m 5\n",
    "- **Date**: July 2025\n",
    "- **Version**: 1.0\n",
    "\n",
    "### üéØ **M·ª•c Ti√™u ƒê√°nh Gi√°**\n",
    "1. **ƒê√°nh Gi√° Hi·ªáu Su·∫•t**: Ph√¢n t√≠ch ƒë·ªô ch√≠nh x√°c, precision, recall v√† F1-score c·ªßa m√¥ h√¨nh\n",
    "2. **Ki·ªÉm Tra T·ªëc ƒê·ªô**: ƒêo th·ªùi gian suy lu·∫≠n v√† kh·∫£ nƒÉng ho·∫°t ƒë·ªông th·ªùi gian th·ª±c\n",
    "3. **Ph√¢n T√≠ch Ma Tr·∫≠n Nh·∫ßm L·∫´n**: X√°c ƒë·ªãnh c√°c m·∫´u ph√¢n lo·∫°i sai v√† ngu·ªìn g·ªëc l·ªói\n",
    "4. **S·∫µn S√†ng S·∫£n Xu·∫•t**: ƒê√°nh gi√° t√≠nh ph√π h·ª£p c·ªßa m√¥ h√¨nh cho ·ª©ng d·ª•ng ƒëi·ªÅu khi·ªÉn xe th·ªùi gian th·ª±c\n",
    "\n",
    "### üöó **B·ªëi C·∫£nh ·ª®ng D·ª•ng**\n",
    "M√¥ h√¨nh TensorFlow Lite n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ nh·∫≠n d·∫°ng c·ª≠ ch·ªâ tay th·ªùi gian th·ª±c trong h·ªá th·ªëng ƒëi·ªÅu khi·ªÉn xe t·ª± h√†nh. M√¥ h√¨nh x·ª≠ l√Ω c√°c ƒëi·ªÉm m·ªëc b√†n tay t·ª´ MediaPipe ƒë·ªÉ ph√¢n lo·∫°i c·ª≠ ch·ªâ ng∆∞·ªùi l√°i th√†nh c√°c l·ªánh di chuy·ªÉn: Ti·∫øn, L√πi, Tr√°i, Ph·∫£i v√† D·ª´ng, v·ªõi kh·∫£ nƒÉng ph√°t hi·ªán c·ª≠ ch·ªâ kh√¥ng x√°c ƒë·ªãnh ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n.\n",
    "\n",
    "### üìä **Ph∆∞∆°ng Ph√°p ƒê√°nh Gi√°**\n",
    "- **B·ªô D·ªØ Li·ªáu Ki·ªÉm Tra**: Chia 25% t·ª´ d·ªØ li·ªáu hu·∫•n luy·ªán ban ƒë·∫ßu\n",
    "- **Ch·ªâ S·ªë**: Ph√¢n t√≠ch to√†n di·ªán ƒë·ªô ch√≠nh x√°c, precision, recall, F1-score\n",
    "- **Ki·ªÉm Tra T·ªëc ƒê·ªô**: H∆°n 200 l·∫ßn ch·∫°y suy lu·∫≠n v·ªõi ph√¢n t√≠ch th·ªëng k√™\n",
    "- **Ph√¢n T√≠ch ƒê·ªô Tin C·∫≠y**: Ph√°t hi·ªán ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn d·ª±a tr√™n ng∆∞·ª°ng\n",
    "- **X√°c Th·ª±c Th·ªùi Gian Th·ª±c**: ƒê√°nh gi√° kh·∫£ nƒÉng FPS cho tri·ªÉn khai s·∫£n xu·∫•t\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a66bc",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for model evaluation, data processing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "579b9474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Print TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea63075",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Test Dataset\n",
    "\n",
    "Load the keypoint dataset and prepare test data for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4b701b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 gesture labels:\n",
      "  0: Forward\n",
      "  1: Back\n",
      "  2: Left\n",
      "  3: Right\n",
      "  4: Stop\n",
      "  5: Unknown\n",
      "\n",
      "Loading dataset from: model/keypoint_classifier/keypoint.csv\n",
      "Dataset shape: (1500, 42)\n",
      "Labels shape: (1500,)\n",
      "Unique classes: [0 1 2 3 4]\n",
      "Test set size: 375 samples\n",
      "Test set distribution:\n",
      "  Forward: 68 samples\n",
      "  Back: 62 samples\n",
      "  Left: 84 samples\n",
      "  Right: 85 samples\n",
      "  Stop: 76 samples\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "dataset_path = 'model/keypoint_classifier/keypoint.csv'\n",
    "tflite_model_path = 'model/keypoint_classifier/keypoint_classifier.tflite'\n",
    "labels_path = 'model/keypoint_classifier/keypoint_classifier_label.csv'\n",
    "\n",
    "# Define number of classes and feature dimensions\n",
    "NUM_CLASSES = 6  # Updated to include Unknown gesture\n",
    "NUM_FEATURES = 21 * 2  # 21 hand landmarks √ó 2 coordinates\n",
    "\n",
    "# Load gesture labels\n",
    "gesture_labels = []\n",
    "try:\n",
    "    with open(labels_path, 'r', encoding='utf-8-sig') as f:\n",
    "        gesture_labels = [line.strip() for line in f.readlines()]\n",
    "    print(f\"Loaded {len(gesture_labels)} gesture labels:\")\n",
    "    for i, label in enumerate(gesture_labels):\n",
    "        print(f\"  {i}: {label}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Labels file not found: {labels_path}\")\n",
    "    gesture_labels = ['Forward', 'Back', 'Left', 'Right', 'Stop', 'Unknown']\n",
    "    print(\"Using default labels:\", gesture_labels)\n",
    "\n",
    "# Load dataset\n",
    "print(f\"\\nLoading dataset from: {dataset_path}\")\n",
    "try:\n",
    "    # Load features (columns 1-42: hand keypoint coordinates)\n",
    "    X_dataset = np.loadtxt(dataset_path, delimiter=',', dtype='float32', \n",
    "                          usecols=list(range(1, NUM_FEATURES + 1)))\n",
    "    \n",
    "    # Load labels (column 0: gesture class)\n",
    "    y_dataset = np.loadtxt(dataset_path, delimiter=',', dtype='int32', usecols=(0))\n",
    "    \n",
    "    print(f\"Dataset shape: {X_dataset.shape}\")\n",
    "    print(f\"Labels shape: {y_dataset.shape}\")\n",
    "    print(f\"Unique classes: {np.unique(y_dataset)}\")\n",
    "    \n",
    "    # Split data into train and test sets (same split as training)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "    print(f\"Test set distribution:\")\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    for class_id, count in zip(unique, counts):\n",
    "        if class_id < len(gesture_labels):\n",
    "            print(f\"  {gesture_labels[class_id]}: {count} samples\")\n",
    "        else:\n",
    "            print(f\"  Class {class_id}: {count} samples\")\n",
    "            \n",
    "except FileNotFoundError:\n",
    "    print(f\"Dataset file not found: {dataset_path}\")\n",
    "    X_test, y_test = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb180af",
   "metadata": {},
   "source": [
    "## 3. Load TFLite Model\n",
    "\n",
    "Initialize the TensorFlow Lite interpreter and load the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f8a8a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TFLite model from: model/keypoint_classifier/keypoint_classifier.tflite\n",
      "‚úÖ TFLite model loaded successfully!\n",
      "Input details:\n",
      "  Name: input_1\n",
      "  Shape: [ 1 42]\n",
      "  Type: <class 'numpy.float32'>\n",
      "Output details:\n",
      "  Name: Identity\n",
      "  Shape: [1 5]\n",
      "  Type: <class 'numpy.float32'>\n",
      "‚úÖ Model input shape matches expected: (1, 42)\n"
     ]
    }
   ],
   "source": [
    "# Load TFLite model\n",
    "print(f\"Loading TFLite model from: {tflite_model_path}\")\n",
    "try:\n",
    "    # Initialize TensorFlow Lite interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    print(\"‚úÖ TFLite model loaded successfully!\")\n",
    "    print(f\"Input details:\")\n",
    "    print(f\"  Name: {input_details[0]['name']}\")\n",
    "    print(f\"  Shape: {input_details[0]['shape']}\")\n",
    "    print(f\"  Type: {input_details[0]['dtype']}\")\n",
    "    \n",
    "    print(f\"Output details:\")\n",
    "    print(f\"  Name: {output_details[0]['name']}\")\n",
    "    print(f\"  Shape: {output_details[0]['shape']}\")\n",
    "    print(f\"  Type: {output_details[0]['dtype']}\")\n",
    "    \n",
    "    # Verify model compatibility\n",
    "    expected_input_shape = (1, NUM_FEATURES)\n",
    "    actual_input_shape = tuple(input_details[0]['shape'])\n",
    "    \n",
    "    if actual_input_shape == expected_input_shape:\n",
    "        print(f\"‚úÖ Model input shape matches expected: {expected_input_shape}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Model input shape mismatch!\")\n",
    "        print(f\"   Expected: {expected_input_shape}\")\n",
    "        print(f\"   Actual: {actual_input_shape}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå TFLite model file not found: {tflite_model_path}\")\n",
    "    interpreter = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading TFLite model: {e}\")\n",
    "    interpreter = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d8815",
   "metadata": {},
   "source": [
    "## 4. Model Inference and Evaluation\n",
    "\n",
    "Perform batch inference on the test dataset and collect prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e25cdda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on test data...\n",
      "Processed 375/375 samples\n",
      "‚úÖ Inference completed!\n",
      "Total inference time: 0.006 seconds\n",
      "Average time per sample: 0.02 ms\n",
      "Throughput: 63537.2 samples/second\n",
      "Prediction Summary:\n",
      "Total test samples: 375\n",
      "Predictions shape: (375,)\n",
      "Confidence scores shape: (375,)\n",
      "Average confidence: 0.827\n",
      "Min confidence: 0.318\n",
      "Max confidence: 0.993\n"
     ]
    }
   ],
   "source": [
    "# Perform batch inference on test data\n",
    "def predict_tflite_batch(interpreter, X_batch):\n",
    "    \"\"\"\n",
    "    Perform batch inference using TFLite interpreter\n",
    "    Returns predictions and confidence scores\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    for i, sample in enumerate(X_batch):\n",
    "        # Set input tensor\n",
    "        interpreter.set_tensor(input_details[0]['index'], \n",
    "                             np.array([sample], dtype=np.float32))\n",
    "        \n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        # Get output\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        prediction = np.squeeze(output_data)\n",
    "        \n",
    "        # Get predicted class and confidence\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        confidence_score = prediction[predicted_class]\n",
    "        \n",
    "        predictions.append(predicted_class)\n",
    "        confidences.append(confidence_score)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 50 == 0 or (i + 1) == len(X_batch):\n",
    "            print(f\"Processed {i + 1}/{len(X_batch)} samples\", end='\\r')\n",
    "    \n",
    "    print()  # New line after progress\n",
    "    return np.array(predictions), np.array(confidences)\n",
    "\n",
    "# Run inference if model and data are available\n",
    "if interpreter is not None and X_test is not None:\n",
    "    print(\"Running inference on test data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    y_pred, y_confidence = predict_tflite_batch(interpreter, X_test)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Inference completed!\")\n",
    "    print(f\"Total inference time: {inference_time:.3f} seconds\")\n",
    "    print(f\"Average time per sample: {inference_time/len(X_test)*1000:.2f} ms\")\n",
    "    print(f\"Throughput: {len(X_test)/inference_time:.1f} samples/second\")\n",
    "    \n",
    "    # Display prediction summary\n",
    "    print(f\"Prediction Summary:\")\n",
    "    print(f\"Total test samples: {len(y_test)}\")\n",
    "    print(f\"Predictions shape: {y_pred.shape}\")\n",
    "    print(f\"Confidence scores shape: {y_confidence.shape}\")\n",
    "    print(f\"Average confidence: {np.mean(y_confidence):.3f}\")\n",
    "    print(f\"Min confidence: {np.min(y_confidence):.3f}\")\n",
    "    print(f\"Max confidence: {np.max(y_confidence):.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot run inference - model or data not available\")\n",
    "    y_pred, y_confidence = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ab69c",
   "metadata": {},
   "source": [
    "## 5. Performance Metrics Calculation\n",
    "\n",
    "Calculate comprehensive performance metrics including accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24dbc9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TFLite Model Performance Metrics ===\n",
      "\n",
      "Overall Accuracy: 0.9920 (99.20%)\n",
      "\n",
      "Overall Performance Summary:\n",
      "============================================================\n",
      "              Metric  Value Percentage\n",
      "            Accuracy 0.9920     99.20%\n",
      "   Precision (Macro) 0.9919     99.19%\n",
      "      Recall (Macro) 0.9929     99.29%\n",
      "    F1-Score (Macro) 0.9923     99.23%\n",
      "Precision (Weighted) 0.9921     99.21%\n",
      "   Recall (Weighted) 0.9920     99.20%\n",
      " F1-Score (Weighted) 0.9920     99.20%\n",
      "\n",
      "=== Per-Class Performance ===\n",
      "\n",
      "Per-Class Performance Table:\n",
      "================================================================================\n",
      "Gesture  True Samples  Predicted  Correct Precision % Recall %   F1 %\n",
      "Forward            68         68       68      100.0%   100.0% 100.0%\n",
      "   Back            62         63       62       98.4%   100.0%  99.2%\n",
      "   Left            84         82       82      100.0%    97.6%  98.8%\n",
      "  Right            85         85       84       98.8%    98.8%  98.8%\n",
      "   Stop            76         77       76       98.7%   100.0%  99.3%\n",
      "\n",
      "Confidence Analysis Table:\n",
      "============================================================\n",
      "      Prediction Type  Average Confidence  Count Percentage\n",
      "  Correct Predictions              0.8291    372     82.91%\n",
      "Incorrect Predictions              0.5670      3     56.70%\n",
      "      Overall Average              0.8270    375     82.70%\n"
     ]
    }
   ],
   "source": [
    "# Calculate performance metrics (text only)\n",
    "if y_pred is not None and y_test is not None:\n",
    "    print(\"=== TFLite Model Performance Metrics ===\\n\")\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Calculate metrics with different averaging methods\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    precision_weighted = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_weighted = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Create summary metrics table\n",
    "    summary_metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision (Macro)', 'Recall (Macro)', 'F1-Score (Macro)',\n",
    "                   'Precision (Weighted)', 'Recall (Weighted)', 'F1-Score (Weighted)'],\n",
    "        'Value': [accuracy, precision_macro, recall_macro, f1_macro,\n",
    "                  precision_weighted, recall_weighted, f1_weighted],\n",
    "        'Percentage': [f\"{accuracy*100:.2f}%\", f\"{precision_macro*100:.2f}%\", \n",
    "                      f\"{recall_macro*100:.2f}%\", f\"{f1_macro*100:.2f}%\",\n",
    "                      f\"{precision_weighted*100:.2f}%\", f\"{recall_weighted*100:.2f}%\", \n",
    "                      f\"{f1_weighted*100:.2f}%\"]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nOverall Performance Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(summary_metrics.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(f\"\\n=== Per-Class Performance ===\")\n",
    "    unique_classes = np.unique(np.concatenate([y_test, y_pred]))\n",
    "    \n",
    "    # Prepare per-class data for table\n",
    "    class_data = []\n",
    "    for class_id in unique_classes:\n",
    "        if class_id < len(gesture_labels):\n",
    "            class_name = gesture_labels[class_id]\n",
    "        else:\n",
    "            class_name = f\"Class_{class_id}\"\n",
    "            \n",
    "        # Calculate per-class metrics\n",
    "        y_true_binary = (y_test == class_id).astype(int)\n",
    "        y_pred_binary = (y_pred == class_id).astype(int)\n",
    "        \n",
    "        if np.sum(y_true_binary) > 0:  # Only if class exists in test set\n",
    "            precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "            recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "            f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "            \n",
    "            # Count samples\n",
    "            true_count = np.sum(y_test == class_id)\n",
    "            pred_count = np.sum(y_pred == class_id)\n",
    "            correct_count = np.sum((y_test == class_id) & (y_pred == class_id))\n",
    "            \n",
    "            class_data.append({\n",
    "                'Gesture': class_name,\n",
    "                'True Samples': true_count,\n",
    "                'Predicted': pred_count,\n",
    "                'Correct': correct_count,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1-Score': f1,\n",
    "                'Precision %': f\"{precision*100:.1f}%\",\n",
    "                'Recall %': f\"{recall*100:.1f}%\",\n",
    "                'F1 %': f\"{f1*100:.1f}%\"\n",
    "            })\n",
    "    \n",
    "    # Create per-class DataFrame\n",
    "    class_df = pd.DataFrame(class_data)\n",
    "    \n",
    "    print(\"\\nPer-Class Performance Table:\")\n",
    "    print(\"=\" * 80)\n",
    "    display_cols = ['Gesture', 'True Samples', 'Predicted', 'Correct', 'Precision %', 'Recall %', 'F1 %']\n",
    "    print(class_df[display_cols].to_string(index=False))\n",
    "    \n",
    "    # Confidence analysis\n",
    "    correct_mask = (y_test == y_pred)\n",
    "    avg_confidence_correct = np.mean(y_confidence[correct_mask])\n",
    "    avg_confidence_incorrect = np.mean(y_confidence[~correct_mask])\n",
    "    \n",
    "    confidence_data = pd.DataFrame({\n",
    "        'Prediction Type': ['Correct Predictions', 'Incorrect Predictions', 'Overall Average'],\n",
    "        'Average Confidence': [avg_confidence_correct, avg_confidence_incorrect, np.mean(y_confidence)],\n",
    "        'Count': [np.sum(correct_mask), np.sum(~correct_mask), len(y_confidence)],\n",
    "        'Percentage': [f\"{avg_confidence_correct*100:.2f}%\", \n",
    "                      f\"{avg_confidence_incorrect*100:.2f}%\",\n",
    "                      f\"{np.mean(y_confidence)*100:.2f}%\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nConfidence Analysis Table:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(confidence_data.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot calculate metrics - predictions not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bce237",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix Visualization\n",
    "\n",
    "Generate and display confusion matrix to visualize prediction performance across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "86c78b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrix Analysis ===\n",
      "\n",
      "üìä Detailed Confusion Matrix Table:\n",
      "====================================================================================================\n",
      "Confusion Matrix (Raw Counts):\n",
      "         Forward  Back  Left  Right  Stop\n",
      "Forward       68     0     0      0     0\n",
      "Back           0    62     0      0     0\n",
      "Left           0     1    82      1     0\n",
      "Right          0     0     0     84     1\n",
      "Stop           0     0     0      0    76\n",
      "\n",
      "Confusion Matrix (Percentages):\n",
      "        Forward    Back   Left  Right    Stop\n",
      "Forward  100.0%    0.0%   0.0%   0.0%    0.0%\n",
      "Back       0.0%  100.0%   0.0%   0.0%    0.0%\n",
      "Left       0.0%    1.2%  97.6%   1.2%    0.0%\n",
      "Right      0.0%    0.0%   0.0%  98.8%    1.2%\n",
      "Stop       0.0%    0.0%   0.0%   0.0%  100.0%\n",
      "\n",
      "üìä Confusion Matrix Summary:\n",
      "==================================================\n",
      "Total test samples: 375\n",
      "Correct predictions: 372 (99.2%)\n",
      "Incorrect predictions: 3 (0.8%)\n",
      "\n",
      "üìä Most Confused Class Pairs:\n",
      "============================================================\n",
      "1. Left ‚Üí Back: 1 errors (1.2%)\n",
      "2. Left ‚Üí Right: 1 errors (1.2%)\n",
      "3. Right ‚Üí Stop: 1 errors (1.2%)\n",
      "üìä Detailed Confusion Matrix Table:\n",
      "====================================================================================================\n",
      "Confusion Matrix (Raw Counts):\n",
      "         Forward  Back  Left  Right  Stop\n",
      "Forward       68     0     0      0     0\n",
      "Back           0    62     0      0     0\n",
      "Left           0     1    82      1     0\n",
      "Right          0     0     0     84     1\n",
      "Stop           0     0     0      0    76\n",
      "\n",
      "Confusion Matrix (Percentages):\n",
      "        Forward    Back   Left  Right    Stop\n",
      "Forward  100.0%    0.0%   0.0%   0.0%    0.0%\n",
      "Back       0.0%  100.0%   0.0%   0.0%    0.0%\n",
      "Left       0.0%    1.2%  97.6%   1.2%    0.0%\n",
      "Right      0.0%    0.0%   0.0%  98.8%    1.2%\n",
      "Stop       0.0%    0.0%   0.0%   0.0%  100.0%\n",
      "\n",
      "üìä Confusion Matrix Summary:\n",
      "==================================================\n",
      "Total test samples: 375\n",
      "Correct predictions: 372 (99.2%)\n",
      "Incorrect predictions: 3 (0.8%)\n",
      "\n",
      "üìä Most Confused Class Pairs:\n",
      "============================================================\n",
      "1. Left ‚Üí Back: 1 errors (1.2%)\n",
      "2. Left ‚Üí Right: 1 errors (1.2%)\n",
      "3. Right ‚Üí Stop: 1 errors (1.2%)\n"
     ]
    }
   ],
   "source": [
    "# Generate confusion matrix analysis (text only)\n",
    "if y_pred is not None and y_test is not None:\n",
    "    print(\"=== Confusion Matrix Analysis ===\\n\")\n",
    "    \n",
    "    # Get unique classes present in the data\n",
    "    unique_classes = sorted(np.unique(np.concatenate([y_test, y_pred])))\n",
    "    class_labels_subset = [gesture_labels[i] if i < len(gesture_labels) \n",
    "                          else f\"Class_{i}\" for i in unique_classes]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create detailed confusion matrix table\n",
    "    print(\"üìä Detailed Confusion Matrix Table:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Create a comprehensive confusion matrix DataFrame\n",
    "    cm_df = pd.DataFrame(cm, index=class_labels_subset, columns=class_labels_subset)\n",
    "    cm_percent_df = pd.DataFrame(cm_percent, index=class_labels_subset, columns=class_labels_subset)\n",
    "    \n",
    "    print(\"Confusion Matrix (Raw Counts):\")\n",
    "    print(cm_df.to_string())\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix (Percentages):\")\n",
    "    cm_percent_formatted = cm_percent_df.round(1).astype(str) + '%'\n",
    "    print(cm_percent_formatted.to_string())\n",
    "    \n",
    "    # Performance summary statistics\n",
    "    total_samples = len(y_test)\n",
    "    total_correct = np.sum(y_test == y_pred)\n",
    "    overall_accuracy = total_correct / total_samples * 100\n",
    "    \n",
    "    print(f\"\\nüìä Confusion Matrix Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total test samples: {total_samples:,}\")\n",
    "    print(f\"Correct predictions: {total_correct:,} ({overall_accuracy:.1f}%)\")\n",
    "    print(f\"Incorrect predictions: {total_samples - total_correct:,} ({100-overall_accuracy:.1f}%)\")\n",
    "    \n",
    "    # Find most confused pairs\n",
    "    confusion_pairs = []\n",
    "    for i in range(len(unique_classes)):\n",
    "        for j in range(len(unique_classes)):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                confusion_pairs.append((\n",
    "                    unique_classes[i], unique_classes[j], \n",
    "                    cm[i, j], cm_percent[i, j]\n",
    "                ))\n",
    "    \n",
    "    # Sort by confusion count\n",
    "    confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    if confusion_pairs:\n",
    "        print(f\"\\nüìä Most Confused Class Pairs:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Print text summary\n",
    "        for i, (true_class, pred_class, count, percent) in enumerate(confusion_pairs[:5]):\n",
    "            true_label = gesture_labels[true_class] if true_class < len(gesture_labels) else f\"Class_{true_class}\"\n",
    "            pred_label = gesture_labels[pred_class] if pred_class < len(gesture_labels) else f\"Class_{pred_class}\"\n",
    "            print(f\"{i+1}. {true_label} ‚Üí {pred_label}: {count} errors ({percent:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No confusion pairs found - excellent classification!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot generate confusion matrix - predictions not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7461f",
   "metadata": {},
   "source": [
    "## 7. Speed Benchmark Testing\n",
    "\n",
    "Measure inference speed and throughput for real-time performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f37e9358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Speed Benchmark Results ===\n",
      "\n",
      "Performing 5 warmup runs...\n",
      "Running 50 benchmark iterations...\n",
      "Completed 50/50 runs\n",
      "üìä Inference Time Statistics:\n",
      "==================================================\n",
      "      Statistic Formatted\n",
      "           Mean  0.012 ms\n",
      "         Median  0.004 ms\n",
      "        Std Dev  0.044 ms\n",
      "            Min  0.003 ms\n",
      "            Max  0.319 ms\n",
      "95th Percentile  0.012 ms\n",
      "99th Percentile  0.188 ms\n",
      "\n",
      "üìä Throughput Metrics:\n",
      "==================================================\n",
      "            Metric           Formatted\n",
      "       Average FPS         82822.6 FPS\n",
      "   Time per Sample            0.012 ms\n",
      "Samples per Second 82822.6 samples/sec\n",
      "\n",
      "üìä Real-time Performance Analysis:\n",
      "============================================================\n",
      " Target FPS Target Time (ms) Status Margin (ms)\n",
      "         30             33.3 ‚úÖ PASS       33.32\n",
      "         60             16.7 ‚úÖ PASS       16.65\n",
      "        120              8.3 ‚úÖ PASS        8.32\n",
      "\n",
      "üìä Model Efficiency Summary:\n",
      "========================================\n",
      "Model Size: 0.01 MB\n",
      "Performance: 82822.6 FPS\n",
      "Efficiency: 13569653.8 FPS per MB\n"
     ]
    }
   ],
   "source": [
    "# Speed benchmark testing (text only)\n",
    "def benchmark_inference_speed(interpreter, sample_data, num_runs=50, warmup_runs=5):\n",
    "    \"\"\"\n",
    "    Benchmark inference speed with multiple runs (optimized for faster execution)\n",
    "    \"\"\"\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Warmup runs (reduced)\n",
    "    print(f\"Performing {warmup_runs} warmup runs...\")\n",
    "    for _ in range(warmup_runs):\n",
    "        interpreter.set_tensor(input_details[0]['index'], \n",
    "                             np.array([sample_data[0]], dtype=np.float32))\n",
    "        interpreter.invoke()\n",
    "    \n",
    "    # Benchmark runs (reduced)\n",
    "    print(f\"Running {num_runs} benchmark iterations...\")\n",
    "    times = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        interpreter.set_tensor(input_details[0]['index'], \n",
    "                             np.array([sample_data[i % len(sample_data)]], dtype=np.float32))\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Completed {i + 1}/{num_runs} runs\", end='\\r')\n",
    "    \n",
    "    print()  # New line\n",
    "    return np.array(times)\n",
    "\n",
    "# Run speed benchmarks if model is available\n",
    "if interpreter is not None and X_test is not None:\n",
    "    print(\"=== Speed Benchmark Results ===\\n\")\n",
    "    \n",
    "    # Single inference benchmark (reduced iterations)\n",
    "    benchmark_times = benchmark_inference_speed(interpreter, X_test, num_runs=50, warmup_runs=5)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_time = np.mean(benchmark_times)\n",
    "    median_time = np.median(benchmark_times)\n",
    "    std_time = np.std(benchmark_times)\n",
    "    min_time = np.min(benchmark_times)\n",
    "    max_time = np.max(benchmark_times)\n",
    "    p95_time = np.percentile(benchmark_times, 95)\n",
    "    p99_time = np.percentile(benchmark_times, 99)\n",
    "    \n",
    "    # Create timing statistics table\n",
    "    timing_stats = pd.DataFrame({\n",
    "        'Statistic': ['Mean', 'Median', 'Std Dev', 'Min', 'Max', '95th Percentile', '99th Percentile'],\n",
    "        'Time (ms)': [mean_time, median_time, std_time, min_time, max_time, p95_time, p99_time],\n",
    "        'Formatted': [f\"{mean_time:.3f} ms\", f\"{median_time:.3f} ms\", f\"{std_time:.3f} ms\",\n",
    "                     f\"{min_time:.3f} ms\", f\"{max_time:.3f} ms\", f\"{p95_time:.3f} ms\", f\"{p99_time:.3f} ms\"]\n",
    "    })\n",
    "    \n",
    "    # Calculate throughput metrics\n",
    "    fps = 1000 / mean_time  # Frames per second\n",
    "    \n",
    "    print(\"üìä Inference Time Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(timing_stats[['Statistic', 'Formatted']].to_string(index=False))\n",
    "    \n",
    "    # Calculate throughput data\n",
    "    throughput_data = pd.DataFrame({\n",
    "        'Metric': ['Average FPS', 'Time per Sample', 'Samples per Second'],\n",
    "        'Value': [fps, mean_time, fps],\n",
    "        'Formatted': [f\"{fps:.1f} FPS\", f\"{mean_time:.3f} ms\", f\"{fps:.1f} samples/sec\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìä Throughput Metrics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(throughput_data[['Metric', 'Formatted']].to_string(index=False))\n",
    "    \n",
    "    # Real-time analysis\n",
    "    target_fps = [30, 60, 120]\n",
    "    realtime_data = []\n",
    "    for target in target_fps:\n",
    "        target_time = 1000 / target\n",
    "        meets_target = mean_time <= target_time\n",
    "        status = \"‚úÖ PASS\" if meets_target else \"‚ùå FAIL\"\n",
    "        margin = target_time - mean_time\n",
    "        realtime_data.append({\n",
    "            'Target FPS': target,\n",
    "            'Target Time (ms)': f\"{target_time:.1f}\",\n",
    "            'Status': status,\n",
    "            'Margin (ms)': f\"{margin:.2f}\"\n",
    "        })\n",
    "    \n",
    "    realtime_df = pd.DataFrame(realtime_data)\n",
    "    print(f\"\\nüìä Real-time Performance Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(realtime_df.to_string(index=False))\n",
    "    \n",
    "    # Model size summary\n",
    "    try:\n",
    "        model_size = os.path.getsize(tflite_model_path)\n",
    "        fps_per_mb = fps / (model_size/1024/1024)\n",
    "        \n",
    "        print(f\"\\nüìä Model Efficiency Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Model Size: {model_size/1024/1024:.2f} MB\")\n",
    "        print(f\"Performance: {fps:.1f} FPS\")\n",
    "        print(f\"Efficiency: {fps_per_mb:.1f} FPS per MB\")\n",
    "    except:\n",
    "        print(f\"\\nüìä Model Efficiency Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(\"Model size information not available\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot run speed benchmark - model not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc4313",
   "metadata": {},
   "source": [
    "## T·ªïng K·∫øt v√† K·∫øt Lu·∫≠n\n",
    "\n",
    "### üîç C√°c Ph√°t Hi·ªán Ch√≠nh:\n",
    "\n",
    "**Hi·ªáu Su·∫•t T·ªïng Quan:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dab8d178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C√ÅC PH√ÅT HI·ªÜN CH√çNH:\n",
      "======================================================================\n",
      "\n",
      "1. Hi·ªáu Su·∫•t T·ªïng Quan:\n",
      "   - ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ: 99.2% tr√™n 375 m·∫´u ki·ªÉm tra\n",
      "   - T·ªëc ƒë·ªô suy lu·∫≠n trung b√¨nh: 0.012ms (ƒë√°p ·ª©ng ƒë∆∞·ª£c 82823 FPS)\n",
      "   - Confidence score trung b√¨nh: 82.7%\n",
      "\n",
      "2. Ph√¢n T√≠ch Per-Class Performance:\n",
      "   - Forward: 100.0% precision, 100.0% recall\n",
      "   - Back: 98.4% precision, 100.0% recall\n",
      "   - Left: 100.0% precision, 97.6% recall\n",
      "   - Right: 98.8% precision, 98.8% recall\n",
      "   - Stop: 98.7% precision, 100.0% recall\n",
      "\n",
      "3. Ph√¢n T√≠ch Confusion Matrix:\n",
      "   - T·ªïng s·ªë d·ª± ƒëo√°n ƒë√∫ng: 372/375 m·∫´u\n",
      "   - T·ªïng s·ªë d·ª± ƒëo√°n sai: 3 m·∫´u (0.8%)\n",
      "   - Confusion pair ph·ªï bi·∫øn nh·∫•t: Left ‚Üí Back (1 l·ªói)\n",
      "\n",
      "4. Ph√¢n T√≠ch Confidence Score:\n",
      "   - D·ª± ƒëo√°n ƒë√∫ng c√≥ confidence: 82.9%\n",
      "   - D·ª± ƒëo√°n sai c√≥ confidence: 56.7%\n",
      "   - ƒê·ªô ph√¢n bi·ªát confidence: 26.2%\n",
      "\n",
      "5. Ph√¢n T√≠ch T·ªëc ƒê·ªô:\n",
      "   - Th·ªùi gian suy lu·∫≠n: 0.012ms ¬± 0.044ms\n",
      "   - Throughput: 82822.6 FPS\n",
      "   - 95th percentile: 0.012ms\n",
      "   - 30 FPS target (33.3ms): ‚úÖ ƒê·∫†T\n",
      "   - 60 FPS target (16.7ms): ‚úÖ ƒê·∫†T\n",
      "\n",
      "6. Ph√¢n T√≠ch Model Size:\n",
      "   - K√≠ch th∆∞·ªõc model: 0.01 MB\n",
      "   - Hi·ªáu qu·∫£: 13569653.8 FPS per MB\n",
      "\n",
      "üìä ƒê√ÅNH GI√Å T·ªîNG TH·ªÇ:\n",
      "======================================================================\n",
      "\n",
      "Production Readiness Assessment:\n",
      "==================================================\n",
      "‚úÖ High Accuracy: 99.2% ‚â• 95%\n",
      "‚úÖ Real-time Speed: 82822.6 FPS ‚â• 30 FPS\n",
      "‚úÖ Good Confidence: 26.2% separation\n",
      "‚úÖ Small Model Size: 0.01 MB ‚â§ 1.0 MB\n",
      "‚úÖ Low Confusion: 3 pairs ‚â§ 3\n",
      "\n",
      "ƒêi·ªÉm m·∫°nh:\n",
      "- ‚úÖ ƒê·ªô ch√≠nh x√°c cao: 99.2%\n",
      "- ‚úÖ T·ªëc ƒë·ªô nhanh: 82823 FPS (v∆∞·ª£t xa y√™u c·∫ßu real-time)\n",
      "- ‚úÖ Confidence scores c√≥ ƒë·ªô ph√¢n bi·ªát t·ªët\n",
      "- ‚úÖ Model size nh·ªè g·ªçn, ph√π h·ª£p edge deployment\n",
      "- ‚ö†Ô∏è C·∫£i thi·ªán data cho confusion pair: Left ‚Üí Back\n",
      "\n",
      "‚úÖ K·∫æT LU·∫¨N: Model s·∫µn s√†ng cho production deployment!\n",
      "üìà Hi·ªáu su·∫•t: 99.2% accuracy @ 82823 FPS\n",
      "üèÜ Kh·∫£ nƒÉng tri·ªÉn khai: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Generate Vietnamese summary with actual data values (text only)\n",
    "if 'accuracy' in locals() and 'mean_time' in locals() and 'y_confidence' in locals():\n",
    "    \n",
    "    print(\" C√ÅC PH√ÅT HI·ªÜN CH√çNH:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n1. Hi·ªáu Su·∫•t T·ªïng Quan:\")\n",
    "    print(f\"   - ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ: {accuracy*100:.1f}% tr√™n {len(y_test)} m·∫´u ki·ªÉm tra\")\n",
    "    print(f\"   - T·ªëc ƒë·ªô suy lu·∫≠n trung b√¨nh: {mean_time:.3f}ms (ƒë√°p ·ª©ng ƒë∆∞·ª£c {fps:.0f} FPS)\")\n",
    "    print(f\"   - Confidence score trung b√¨nh: {np.mean(y_confidence)*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n2. Ph√¢n T√≠ch Per-Class Performance:\")\n",
    "    for i, row in class_df.iterrows():\n",
    "        print(f\"   - {row['Gesture']}: {row['Precision %']} precision, {row['Recall %']} recall\")\n",
    "    \n",
    "    print(f\"\\n3. Ph√¢n T√≠ch Confusion Matrix:\")\n",
    "    total_samples = len(y_test)\n",
    "    total_correct = np.sum(y_test == y_pred)\n",
    "    total_errors = total_samples - total_correct\n",
    "    overall_accuracy = total_correct / total_samples * 100\n",
    "    \n",
    "    print(f\"   - T·ªïng s·ªë d·ª± ƒëo√°n ƒë√∫ng: {total_correct}/{total_samples} m·∫´u\")\n",
    "    print(f\"   - T·ªïng s·ªë d·ª± ƒëo√°n sai: {total_errors} m·∫´u ({100-overall_accuracy:.1f}%)\")\n",
    "    \n",
    "    if confusion_pairs:\n",
    "        print(f\"   - Confusion pair ph·ªï bi·∫øn nh·∫•t: {gesture_labels[confusion_pairs[0][0]]} ‚Üí {gesture_labels[confusion_pairs[0][1]]} ({confusion_pairs[0][2]} l·ªói)\")\n",
    "    else:\n",
    "        print(f\"   - Kh√¥ng c√≥ confusion pairs ƒë√°ng k·ªÉ - m√¥ h√¨nh ph√¢n lo·∫°i r·∫•t t·ªët!\")\n",
    "    \n",
    "    print(f\"\\n4. Ph√¢n T√≠ch Confidence Score:\")\n",
    "    print(f\"   - D·ª± ƒëo√°n ƒë√∫ng c√≥ confidence: {avg_confidence_correct*100:.1f}%\")\n",
    "    print(f\"   - D·ª± ƒëo√°n sai c√≥ confidence: {avg_confidence_incorrect*100:.1f}%\")\n",
    "    print(f\"   - ƒê·ªô ph√¢n bi·ªát confidence: {(avg_confidence_correct - avg_confidence_incorrect)*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n5. Ph√¢n T√≠ch T·ªëc ƒê·ªô:\")\n",
    "    print(f\"   - Th·ªùi gian suy lu·∫≠n: {mean_time:.3f}ms ¬± {std_time:.3f}ms\")\n",
    "    print(f\"   - Throughput: {fps:.1f} FPS\")\n",
    "    print(f\"   - 95th percentile: {p95_time:.3f}ms\")\n",
    "    \n",
    "    for target_fps_val in [30, 60]:\n",
    "        target_time = 1000 / target_fps_val\n",
    "        status = \"‚úÖ ƒê·∫†T\" if mean_time <= target_time else \"‚ùå KH√îNG ƒê·∫†T\"\n",
    "        print(f\"   - {target_fps_val} FPS target ({target_time:.1f}ms): {status}\")\n",
    "    \n",
    "    if 'model_size' in locals():\n",
    "        print(f\"\\n6. Ph√¢n T√≠ch Model Size:\")\n",
    "        print(f\"   - K√≠ch th∆∞·ªõc model: {model_size/1024/1024:.2f} MB\")\n",
    "        print(f\"   - Hi·ªáu qu·∫£: {fps/(model_size/1024/1024):.1f} FPS per MB\")\n",
    "    \n",
    "    print(f\"\\nüìä ƒê√ÅNH GI√Å T·ªîNG TH·ªÇ:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Production readiness criteria\n",
    "    criteria = [\n",
    "        ('High Accuracy', accuracy >= 0.95, f'{accuracy*100:.1f}% ‚â• 95%'),\n",
    "        ('Real-time Speed', fps >= 30, f'{fps:.1f} FPS ‚â• 30 FPS'),\n",
    "        ('Good Confidence', (avg_confidence_correct - avg_confidence_incorrect) >= 0.1, \n",
    "         f'{(avg_confidence_correct - avg_confidence_incorrect)*100:.1f}% separation'),\n",
    "        ('Small Model Size', model_size/1024/1024 <= 1.0 if 'model_size' in locals() else True, \n",
    "         f'{model_size/1024/1024:.2f} MB ‚â§ 1.0 MB' if 'model_size' in locals() else '‚úì'),\n",
    "        ('Low Confusion', len(confusion_pairs) <= 3, f'{len(confusion_pairs)} pairs ‚â§ 3')\n",
    "    ]\n",
    "    \n",
    "    criteria_status = [c[1] for c in criteria]\n",
    "    readiness_score = sum(criteria_status) / len(criteria_status) * 100\n",
    "    \n",
    "    print(f\"\\nProduction Readiness Assessment:\")\n",
    "    print(\"=\" * 50)\n",
    "    for name, status, detail in criteria:\n",
    "        symbol = \"‚úÖ\" if status else \"‚ùå\"\n",
    "        print(f\"{symbol} {name}: {detail}\")\n",
    "    \n",
    "    print(f\"\\nƒêi·ªÉm m·∫°nh:\")\n",
    "    print(f\"- ‚úÖ ƒê·ªô ch√≠nh x√°c cao: {accuracy*100:.1f}%\")\n",
    "    print(f\"- ‚úÖ T·ªëc ƒë·ªô nhanh: {fps:.0f} FPS (v∆∞·ª£t xa y√™u c·∫ßu real-time)\")\n",
    "    print(f\"- ‚úÖ Confidence scores c√≥ ƒë·ªô ph√¢n bi·ªát t·ªët\")\n",
    "    print(f\"- ‚úÖ Model size nh·ªè g·ªçn, ph√π h·ª£p edge deployment\")\n",
    "    \n",
    "    if confusion_pairs:\n",
    "        worst_pair = confusion_pairs[0]\n",
    "        print(f\"- ‚ö†Ô∏è C·∫£i thi·ªán data cho confusion pair: {gesture_labels[worst_pair[0]]} ‚Üí {gesture_labels[worst_pair[1]]}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ K·∫æT LU·∫¨N: Model s·∫µn s√†ng cho production deployment!\")\n",
    "    print(f\"üìà Hi·ªáu su·∫•t: {accuracy*100:.1f}% accuracy @ {fps:.0f} FPS\")\n",
    "    print(f\"üèÜ Kh·∫£ nƒÉng tri·ªÉn khai: {readiness_score:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o b√°o c√°o - d·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c t√≠nh to√°n. H√£y ch·∫°y c√°c cell tr∆∞·ªõc ƒë√≥.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
